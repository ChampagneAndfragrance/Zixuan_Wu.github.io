@misc{Learning_Diverse_Robot_Striking_Motions_with_Diffu,
 abstract = {Advances in robot learning have enabled robots to generate skills for a variety of tasks. Yet, robot learning is typically sample inefficient, struggles to learn from data sources exhibiting varied behaviors, and does not naturally incorporate constraints. These properties are critical for fast, agile tasks such as playing table tennis. Modern techniques for learning from demonstration improve sample efficiency and scale to diverse data, but are rarely evaluated on agile tasks. In the case of reinforcement learning, achieving good performance requires training on high-fidelity simulators. To overcome these limitations, we develop a novel diffusion modeling approach that is offline, constraint-guided, and expressive of diverse agile behaviors. The key to our approach is a kinematic constraint gradient guidance (KCGG) technique that computes gradients through both the forward kinematics of the robot arm and the diffusion model to direct the sampling process. KCGG minimizes the cost of violating constraints while simultaneously keeping the sampled trajectory in-distribution of the training data. We demonstrate the effectiveness of our approach for time-critical robotic tasks by evaluating KCGG in two challenging domains: simulated air hockey and real table tennis. In simulated air hockey, we achieved a 25.4% increase in block rate, while in table tennis, we saw a 17.3% increase in success rate compared to imitation learning baselines.},
 author = {Kin Man Lee and Sean Ye and Qingyu Xiao and Zixuan Wu and Zulfiqar Zaidi and David B D'Ambrosio and Pannag R Sanketi and Matthew Gombolay},
 citation = {IEEE International Conference on Robotics and Automation (ICRA), 2025},
 conference = {2025 IEEE International Conference on Robotics and Automation (ICRA)},
 title = {Learning Diverse Robot Striking Motions with Diffusion Models and Kinematically Constrained Gradient Guidance}
}
